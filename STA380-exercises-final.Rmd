---
title: "STA 380, Part 2 Exercises"
author: "Yiqun Tian, Yuhan Yin, Alexandre NICOLAÏ, Soumya Swarupa Nayak"
date: "8/10/2020"
output: pdf_document
---

# Visual Story Telling Part 1: Green Buildings 

## Introduction
We tried to understand if the recommendation made by the member of the team is 
based on a strong relationship between the amount of the rent and the green status
or if there are some confounding variables. 
```{r}
rm(list=ls())
# import library 
library(ggplot2)
library(LICORS)  
library(foreach)
library(mosaic)

# Read Data 
green = read.csv('greenbuildings.csv')
summary(green)
```
Here we can have a better idea of the spreads belongs the buildings within each features. 
We can observe large spreads in the total square footage of available rental space in the building, 
the rent charged per square foot, the occupancy rate, the age of the building, 
the electricity costs and the gas costs.


Therefore, we decided to separate the dataset in two. The buildings that have the 
green status are differentiated from the other ones. 
```{r}
# Extract the buildings with green ratings
green_only = subset(green, green_rating==1)
dim(green_only)
```
We now have a dataset composed of 685 observations against 7894 previously.

## Understand the relationship between the rent and the green status

### Relationship between the features

We then analyzed this subset through pairwise between the features. 

First we start with all the features : 

```{r}
pairs(green_only)
```

That way we already identify some relationships between the features. 

We decided to look closer at the following ones: 

Size and Stories / Rent and cluster_rent / cd.total.07 and hd.total07/
hd.total07 and total.dd.07.


Size and Stories: 

```{r}
pairs(~size + stories, green_only)
```

We identified a positive correlation between size and stories 
•	size: the total square footage of available rental space in the building.
•	stories: the height of the building in stories.

The total square footage of available rental space in the building becomes more
important with the heigh of the building in stories. 


Rent and cluster_rent:

```{r}
pairs(~Rent + cluster_rent, green_only)
```

Here we understand that the rent charged to the tenants in the building will depend
of the location. 


cd.total.07 and hd.total07

```{r code5, echo=FALSE}
pairs(~cd_total_07 + hd_total07, green_only)
```

This pairwise reveal an obvious negative relationship. The days a green building needs
cooling don't needs heating. 

Through pairwise we better understand the relationships between the features and 
now we will try to understand what features can influence the amount of the rent 
to know if investing based on the median market rent is a good recommandation. 


### Simple linear regression

```{r code6, echo=FALSE}
lm.fit_size=lm(Rent~size, data=green_only)
summary(lm.fit_size)
```

The size feature is not significant to explain the amount of the rent paid by the tenants.


```{r code7, echo=FALSE}
lm.fit_stories=lm(Rent~stories, data=green_only)
summary(lm.fit_stories)
```

Neither are the stories.


```{r code8, echo=FALSE}
lm.fit_cluster_rent=lm(Rent~cluster_rent, data=green_only)
summary(lm.fit_cluster_rent)
```

But the average rent per square foot per calendar year in the building's local market 
is very significant. It tends to explain very well the rent. 


```{r code9, echo=FALSE}
lm.fit_cd_total=lm(Rent~cd_total_07, data=green_only)
summary(lm.fit_cd_total)
```


The same for the number of cooling degree days in the building's region in 2007. 
The correlation is negative, the rent tend to diminish if there are enough days 
that allow to do not use the cooling.


```{r code10, echo=FALSE}
lm.fit_hd_total=lm(Rent~hd_total07, data=green_only)
summary(lm.fit_hd_total)
```

This result can be interprated identicaly but for hot days. If a building is in 
an area where there are enough hot days, that avoid to use heating, the rent paid 
will be lower.

So we understand that the rent for the green buildings is very sensible to the location 
of the building and especially to the weither for energy consumption purpose. 

To interprete visually these results, we decided to plot the simple linear regression 
of all the features to explain the rent:

```{r fig2, echo=FALSE, fig.align='left', fig.height = 10, fig.width = 10}
library(GGally)
require(ggplot2)
require(reshape2)
green_only2 = melt(green_only, id.vars= 'Rent')
ggplot(green_only2) +
  geom_jitter(aes(value,Rent, colour=variable),) + geom_smooth(aes(value,Rent, colour=variable), method=lm, se=FALSE) +
  facet_wrap(~variable, scales="free_x") +
  labs(x = "Variable", y = "Rent")
```

Here we can clearly confirm visually that the rent is sensible to the weither
and therefore to the price of the electricity.

It seems that heating and cooling in these buildings is done by electricity.

The rent price higher if the building is in an area with a few number of cooling/heating 
days since the resident will consume more electricity.

Seems like the price of the rent is dependent on weither the building will need
heating or cooling and therefore the cost of electricity. 

Here we can think that the surplus per footage square on the green building find 
its explanation from the consumption in electricity. They may be in areas that 
needs where the buildings needs heating or cooling. 

In this case, the rent would be higher but it wouldn't mean more profits for 
the investor. 

We did a multiple linear regression to understand better the interactions between 
the features with the rent. 


### Multiple linear regression

We tried to understand which feature has the most impact on the rent when using all the 
variables:

```{r code11, echo=FALSE}
lm.fit=lm(Rent~.,data=green_only)
summary(lm.fit)
```

Like we have seen in the simple linear regression, the cluster rent to to be the
most important feature to explain the rent. Therefore the location of the building
is what matters the most when investing. 

We tried to remove this feature to highlight the other ones that can be significant 
for the rent:


```{r code12, echo=FALSE}
lm.fit=lm(Rent~.-cluster_rent,data=green_only)
summary(lm.fit)
```

The electricity costs becomes much more significant, as the precipitation, the 
number of days without heating/cooling needs, the employment growth and the 
cluster. 

## Conclusion 

Therefore we believe that the recommendation is a mistake. Indeed, he just based
his decision on the median rent market, comparing between the green buildings 
and the ones without this status. 

We believe that before investing, it is extremely important to evaluate the range
price of the building's local market first. Then to understand the climate of 
the region because the price rent is very sensible to the costs of electricity. 

It is counter intuitive for a green building but it seems that the green status
doesn't necessarily means that the energy consumption is no more an important
factor to take into account when investing. On the contrary it appears that the size
of the available space to rent is way less important than those energetic factor 
linked to the climate of the region. 


# Visual story telling part 2: flights at ABIA
```{r}
rm(list=ls())
# import library 
library(ggplot2)

# Read Data 
flight = read.csv('ABIA.csv', header=TRUE)
head(flight)
```

## When is a good time to fly to minize delay?  
We know flight delay is frustrating. Either waiting to pick-up our loved ones or departing for a business trip, we want our flights to arrive and depart on time. We are interested in finding which flights have a higher possibility of delay, choosing over 30 minutes as significant delay time. 

```{r}
delay_arri = subset(flight, ArrDelay>=30)
delay_depart = subset(flight, DepDelay >=30)

ggplot(data = delay_arri) + 
  geom_point(mapping = aes(x = ArrTime, y = ArrDelay)) +
  labs(title = "Total Delay Arrival in 2018")

ggplot(data = delay_depart) + 
  geom_point(mapping = aes(x = DepTime, y = DepDelay)) +
  labs(title = "Total Delay Departure in 2018")
```

In 2018, the majority of arrival time and departure time in Austin-Bergstrom Interational Airport happened between 6:00 - 2:00. We consider delay over 30 minutes is significant, and delay arrival and delay departure are spread out over the day. At 6:00 - 8:00, frequency of delay arrival and delay departure is relatively low. 

```{r}
ggplot(delay_arri, aes(ArrTime, ArrDelay)) +
  geom_point(aes(color = Origin)) +
  geom_smooth(se = FALSE) +
  labs(title = "Total Delay Arrival in 2018")

ggplot(delay_depart, aes(DepTime, DepDelay)) +
  geom_point(aes(color = Origin)) +
  geom_smooth(se = FALSE) +
  labs(title = "Total Delay Departure in 2018")

```
To further explore the the reason of delay, we look into the origin of arrival and departure. However, this plot is not very representative to tell us which origin has a bigger chance of delay. 

## Which months of a year are best to fly to minimize delay? 
```{r}
delay_flight= flight%>%
mutate(dep_type=ifelse(DepDelay<5, "on time", "delayed"))
qplot(x=Month, fill=dep_type, data=delay_flight, geom="bar", main="Delayed Frequence")
```

We define delay time less than 5 minutes to be on time. We can see March, May, June, July, August and December have a higher travel frequency and higher delayed frequency. We assume during spring break, summer break and winter break, people tend to travel more. On the other hand, to avoid flights delay, we would recommend traveler to fly in September, October and November. 

## Choosing Carriers

```{r}
ggplot(flight, aes(UniqueCarrier))+
   geom_bar()
```
In our first overview, we can see AA, WN, and CO are the most popular carriers. 

```{r}
cancel_flight = flight %>% filter(Cancelled == 1)
qplot(x=UniqueCarrier, fill=CancellationCode, data=cancel_flight, geom="bar", main="Canceled Frequence")
```
Cancellation Code: A = Carrier, B = Weather, C = NAS

While weather is an uncontrollable situation, we can see over all AA, MQ and WN have the highest canceled frequency due to carrier itself. 

```{r}
# Cancellation 
d1 = flight %>%
  group_by(UniqueCarrier) %>%
  summarize(Cancelled_rate = sum(Cancelled=='1')/n())
d1

ggplot(data = d1) + 
  geom_bar(mapping = aes(x=UniqueCarrier, y=Cancelled_rate), stat='identity')
```
Here, we can see MQ and AA have the highest cancelled rate.  

```{r}
# Aggregate thedelay times 
d1 = aggregate(delay_depart$Year ~ delay_depart$UniqueCarrier, delay_depart, count)
d2 = aggregate(delay_depart$DepDelay ~ delay_depart$UniqueCarrier, delay_depart, sum)
merged.df = merge(d1, d2, by = "delay_depart$UniqueCarrier", sort = TRUE)
merged.df

names(merged.df)[1] <- "UniqueCarrier"
names(merged.df)[2] <- "Delay_Times"
names(merged.df)[3] <- "Delay_Length"

ggplot(merged.df, aes(Delay_Times, Delay_Length)) +
  geom_point(aes(color = UniqueCarrier)) +
  geom_smooth(se = FALSE)

# bad carrier
bad.carrier = merged.df[which(merged.df$Delay_Times > 800),]
bad.carrier

# good carrier
good.carrier = merged.df[which(merged.df$Delay_Times <100),]
good.carrier
```
Although AA, WN and CO are the most popular carriers, they are not the best carriers in our choice. We suggest travelers to avoid AA, CO, WN for potential delay and MQ for potential cancellation. We recommend travelers to fly with NW and US. 


# Portfolio modeling

```{r, warning=FALSE, message=FALSE}
rm(list=ls())
library(quantmod)
library(foreach)
library(mosaic)
```

```{r, warning=FALSE}
options("getSymbols.warning4.0"=FALSE)

# Import a few stocks
mystocks = c("DIA", "SPY", "GLD", "XLF", "IYW", "XTL", "USCI", "USO", "DBB") 
P1 = c("DIA", "SPY", "GLD")  # safe 
P2 = c("XLF", "SPY", "GLD", "IYW", "XTL")  # diverse, both index and equity
P3 = c("USCI", "USO", "DBB")  # aggressive, Commodities 
myprices = getSymbols(mystocks, from = "2015-01-01")

for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

# Combine all the returns in a matrix for each portfolio
return1 = cbind(	ClCl(DIAa),	ClCl(SPYa), ClCl(GLDa))
return2 = cbind(	ClCl(XLFa),	ClCl(SPYa), ClCl(GLDa), ClCl(IYWa), ClCl(XTLa))
return3 = cbind(	ClCl(USCIa),	ClCl(USOa), ClCl(DBBa))
return1 = as.matrix(na.omit(return1))
return2 = as.matrix(na.omit(return2))
return3 = as.matrix(na.omit(return3))

# Simulate many different possible futures of each random day
# equally distribution in each portfolio
initial_wealth = 10000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.33, 0.33, 0.34)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(return1, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wt1 = wealthtracker
}

sim2 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(return2, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wt2 = wealthtracker
}

sim3 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.33, 0.33, 0.34)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(return3, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wt3 = wealthtracker
}

# each row is a simulated trajectory
# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30)
hist(sim2[,n_days]- initial_wealth, breaks=30)
hist(sim3[,n_days]- initial_wealth, breaks=30)
# mean
mean1 = mean(sim1[,n_days] - initial_wealth)
mean2 = mean(sim2[,n_days] - initial_wealth)
mean3 = mean(sim3[,n_days] - initial_wealth)
cat("The mean profit/loss for three portfolios are", c(mean1, mean2, mean3), ".\n")
# 5% value at risk:
VaR1 = quantile(sim1[,n_days]- initial_wealth, prob=0.05)
VaR2 = quantile(sim2[,n_days]- initial_wealth, prob=0.05)
VaR3 = quantile(sim3[,n_days]- initial_wealth, prob=0.05)
cat("The 5% value at risk for three portfolios are", c(-VaR1, -VaR2, -VaR3), ".\n")

```

Portfolio 1 consists of "Large Cap Growth Equities" and "Precious Metals" ETF which is a safe choice.
Portfolio 2 consists of "Large Cap Growth Equities", "Precious Metals", "Sector ETF" including financial, technology and communications equities. It is more diverse.
Portfolio 3 consists of "Commodities" ETF so that it is more aggressive.

From the histograms, P1 looks like normal distribution, P2 is more centralized at 0 with higher profit, while P3 has higher volatility ranging from -3600 to 2200 and induces a loss. 

The VaR shows the worst-case scenario that the most you can - with a 95% level of confidence - expect to lose in dollars over the next four weeks. 
P1 is safe with 524.4 loss. 
P2's risks are diversified so it's VaR is not that high, 662.9. 
P3, the most aggressive one, is likely to has the biggest loss of 1062.0.



# Market segmentation
```{r}
rm(list=ls())
# Loading Libraries
library(ggplot2)
library(LICORS)  
library(foreach)
library(mosaic)
# Read Data 
market = read.csv('social_marketing.csv', header=TRUE)
summary(market)
```

```{r}
X = market[,-(36:37)]# Given a smaller set of data from engine 
X = X[,-(1)]
X = X[,-(5)]
X = scale(X, center=TRUE, scale=TRUE) 
```

We considered "uncategorized", "spam" and "adult" columns are unrelated to understand social-media audience. Given a smaller set of data can help us better understand which market segment stands out. 

## Choosing K 
```{r}
#k elbow
library(foreach)
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=50)
  cluster_k$tot.withinss
}
plot(k_grid, SSE_grid)

# CH index
N = nrow(X)
k_grid = seq(2, 20, by=1)
CH_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=50)
  W = cluster_k$tot.withinss
  B = cluster_k$betweenss
  CH = (B/W)*((N-k)/(k-1))
  CH
}

plot(k_grid, CH_grid)
# From above two plots, k = 10 prob a good estimate 
```
From Elbow plot above, we can see k = 6, k = 7 and k = 8 are reasonable choices. 
From CH plot, K = 2 and k = 3 are reasonable options. 
For better interpretation, we choose k = 6. 

## K-mean
```{r}
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

# Run k-means with 6 clusters and 25 starts
set.seed(123)
clust1 = kmeans(X, 6, nstart=25)
clust1$center
```
Z-Score scale cluster is not helpful to inperpret our data. We return to our unscale version to look at each group of clusters. 
```{r}
clust1$center[1,]*sigma + mu 
```
In cluster1, this group actively talks about every topics. All the topics are in the average rank of overall market. They are especially interested in religion, parenting, sport_fandom, food, photo_sharing and family. We consider this cluster as "Family-oriented". 
```{r}
clust1$center[2,]*sigma + mu
```
In cluster 2, we categorized this group as "College Students". This group tends to focus on topics about college_uni, online_gaming. 

```{r}
clust1$center[3,]*sigma + mu
```
In cluster 3, we can see this group is interested in politics, news, sports_fandom and current_events. We categorized this group as "Politician". 
```{r}
clust1$center[4,]*sigma + mu
```
In cluster 4, this group is considered as "Health & Fitness". These people enjoy talking about health_nutrition, personal fitness, cooking, and outdoors. 
```{r}
clust1$center[5,]*sigma + mu
```
In cluster 5, we can see this group of people centers in cooking, photo_sharing, fashion, beauty, health_nutrition and shopping. We consider this group of people might be "Teenage Girls". 
```{r}
clust1$center[6,]*sigma + mu
```
In cluster 6, we don't see this group of people are especially interested in any topics. We consider this group as "Outliers". 
```{r}
qplot(parenting, sports_fandom, data=market, color=factor(clust1$cluster))
qplot(college_uni, online_gaming, data=market, color=factor(clust1$cluster))
qplot(politics, news, data=market, color=factor(clust1$cluster))
qplot(health_nutrition, personal_fitness, data=market, color=factor(clust1$cluster))
qplot(beauty, fashion, data=market, color=factor(clust1$cluster))

```
To confirm our observation, we plot each cluster based on the most important features. To better proven our theories, we use kmeans++ as a different initialization strategy. 

## K-means++ 
```{r}
# Using kmeans++ initialization
clust2 = kmeanspp(X, k=6, nstart=25)

set.seed(123)
clust2$center[1,]*sigma + mu
```
This is our "Outliers" group. They tend to not show particular interest in any topic.

```{r}
clust2$center[2,]*sigma + mu
```
This cluster falls into "College Students". They are interested in college_uni and online_gaming topics. 

```{r}
clust2$center[3,]*sigma + mu
```
This cluster falls into "Health & Fitness", who care about health_nutrition, personal_fitness and cooking. 
```{r}
clust2$center[4,]*sigma + mu
```
This cluster falls into cluster 5 - "Teenage Girls", who enjoy topics like cooking, fashion, beauty and photo_sharing.

```{r}
clust2$center[5,]*sigma + mu
```
We can see this cluster falls into cluster 1 - "Family-Oriented", who is actively talking about religion, sport_fandom, parenting and food.  

```{r}
clust2$center[6,]*sigma + mu
```
This cluster falls into cluster 3 - "Politician", who care about politics and news. 


## Conclusion
From our K-means and K-means++ method, we can conclude that our grouping method works very well in identifying distinct market segments. Considering "NutrientH20" could be a nutrient brand, we recommend "Health & Fitness" segment could be the target audience. This group shares the same interest in health_nutrition, personal_fitness and cooking. "NutrientH20" can focus on its marketing campaign in these related topics. 


# Author attribution

## Pre-processing
In the pre-processing stage, we make everything lowercase, remove numbers, remove punctuation, remove excess white-space, remove stop words. Then we convert the corpus to Document Term Matrix, and remove Sparse terms.

We apply this process function on training and testing data sets. And convert them to TF-IDF weights of each terms. In order to deal with words in the test set that we never saw in the training set, we filter their common terms and just dig into them.

```{r, warning=FALSE}
rm(list=ls())
library(tm)
set.seed(1)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), id=fname, language='en') 
}

# extract all author labels 
authors = Sys.glob('../data/ReutersC50/C50train/*')
  file_list= NULL
  labels = NULL
  # append all txt into one list
  for(author in authors) {
  	author_name = substring(author, first=29)
  	files_to_add = Sys.glob(paste0(author, '/*.txt'))
  	file_list = append(file_list, files_to_add)
  	labels = append(labels, rep(author_name, length(files_to_add)))
  }
  
  all_docs = lapply(file_list, readerPlain) 
  names(all_docs) = file_list
  names(all_docs) = sub('.txt', '', names(all_docs))

# main pre-process function
process = function(x){
  file_list= NULL
  labels = NULL
  # append all txt into one list
  for(author in x) {
  	author_name = substring(author, first=29)
  	files_to_add = Sys.glob(paste0(author, '/*.txt'))
  	file_list = append(file_list, files_to_add)
  	labels = append(labels, rep(author_name, length(files_to_add)))
  }
  
  all_docs = lapply(file_list, readerPlain) 
  names(all_docs) = file_list
  names(all_docs) = sub('.txt', '', names(all_docs))
  my_corpus = Corpus(VectorSource(all_docs))
  
  # Pre-processing
  my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
  my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
  my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
  my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
  my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))
  
  DTM = DocumentTermMatrix(my_corpus)
  DTM = removeSparseTerms(DTM, 0.975)
  
  return(DTM)
}

train = process(Sys.glob('../data/ReutersC50/C50train/*'))
train_tfidf = weightTfIdf(train)
X_train = as.data.frame(as.matrix(train_tfidf))
X_train = cbind(X_train, labels)

test = process(Sys.glob('../data/ReutersC50/C50test/*'))
test_tfidf = weightTfIdf(test)
X_test = as.data.frame(as.matrix(test_tfidf))
X_test = cbind(X_test, labels)

# We just care about common terms in both train and test datasets
common=intersect(names(X_train),names(X_test))
X_train=X_train[,common]
X_test=X_test[,common]

# add "_c" to avoid column name error during model building
colnames(X_train) <- paste(colnames(X_train), "_c",sep = "")
colnames(X_test) <- paste(colnames(X_test), "_c",sep = "")

# change labels to factor
X_train$labels_c = factor(X_train$labels_c) 
X_test$labels_c = factor(X_test$labels_c) 
```

After all data processing steps, we build two models.

## Model 1: Random Forest
```{r}
library(randomForest)
set.seed(1)
for(i in c(100,500,1000)){
  rf_model = randomForest(labels_c~., data = X_train, ntree =i)
  varImpPlot(rf_model)
  
  # predict using X_test
  result = as.data.frame(table(predict(rf_model, X_test, type = "response"),labels))
  # compute the accuracy
  result$correct = ifelse(result$Var1==result$labels, 1, 0)
  result$correct_num = result$Freq * result$correct
  accuracy = sum(result$correct_num)/sum(result$Freq) * 100 
  cat("ntree =", i ,", Accuracy is", accuracy,"%. ")
}
```

## Model 2: Naive Bayes
```{r}
library(tm)
library(e1071)
#library(naivebayes)
NB=naiveBayes(labels_c~., data=X_train)

# predict using X_test
result = as.data.frame(table(predict(NB, X_test, type = "class"),labels))
# compute the accuracy
result$correct = ifelse(result$Var1==result$labels, 1, 0)
result$correct_num = result$Freq * result$correct
accuracy = sum(result$correct_num)/sum(result$Freq) * 100 
cat("Accuracy is", accuracy,"%. ")

```

## Conclusion
Random Forest model performs better than Naive Bayes. 
The best accuracy of random forest models is 62.04% when ntree = 1000



# Association rule mining
```{r}
rm(list=ls())
library(arules)
library(arulesViz)
library(tidyverse)


#read in groceries.txt as transaction 
groceries = read.transactions("groceries.txt", sep = ",")

#summary of our transaction
summary(groceries)
```

```{r}
#plot a top 20 groceries item 
itemFrequencyPlot(groceries,topN=20)

```
We can see 10 most frequent groceries are whole milk, other vegetables, rolls/buns, soda, yogurt, bottled water, root vegetables, tropical fruit shopping bags and sausage. 

```{r}
# Look at rules with support > .005 & confidence >.1 & length (# artists) <= 5
groceries_rule = apriori(groceries, 
	parameter=list(support=.005, confidence=.1, maxlen=5))

# look at output 
inspect(groceries_rule)

# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(groceries_rule)
```

```{r}
## Choose a subset
inspect(subset(groceries_rule, subset=lift > 3))
```


```{r}
lifts <- sort (groceries_rule, by="lift", decreasing=TRUE)
inspect((lifts))[1:15,]

```

```{r}
# can swap the axes and color scales
plot(groceries_rule, measure = c("support", "lift"), shading = "confidence")

# inspect subsets
inspect(subset(groceries_rule, subset=lift > 2.5))
inspect(subset(groceries_rule, subset=confidence > 0.4))
inspect(subset(groceries_rule, subset=lift > 2.5 & confidence > 0.4))

# "two key" plot: coloring is by size (order) of item set
plot(groceries_rule, method='two-key plot')

# can now look at subsets driven by the plot
inspect(subset(groceries_rule, support > 0.035))
inspect(subset(groceries_rule, confidence > 0.6))

# graph-based visualization
sub = subset(groceries_rule, subset=confidence > 0.01 & support > 0.005)
summary(sub1)

plot(sub, method='graph')

plot(head(sub, 30, by='lift'), method='graph')
```
## Conclusion
One interesting finding is that hygiene articles and napkins have a high lift score. This makes sense because people who care more about hygiene would tend to buy more napkins. Ham and white bread also have a high lift score, because people normally need them both to make sandwich. 

Plotting the relationship between confidence and support, we find high lift rules tend to have low support. In the top-30 rules, we can also see a high density of same category merchandises are clustered together. 
